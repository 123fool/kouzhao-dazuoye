# -*- coding: utf-8 -*-
"""
å£ç½©æ£€æµ‹è¯¾ç¨‹ä½œä¸š - ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¸ä¾èµ–numpyçš„å…¨å±€å¯¼å…¥ï¼‰
åŒ…å«ï¼š
1. æ•°æ®é›†æ„å»ºï¼ˆä»XMLæ ‡æ³¨æ„å»ºåˆ†ç±»æ•°æ®é›†ï¼‰
2. ResNet18æ¨¡å‹è®­ç»ƒï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
3. å®Œæ•´çš„æ€§èƒ½è¯„ä¼°

ä½¿ç”¨è¯´æ˜ï¼š
1. ä¿®æ”¹ ROOT å˜é‡ä¸ºä½ çš„æ•°æ®è·¯å¾„
2. è¿è¡Œï¼špython mask_detection_simple.py
"""

import os
import sys
import shutil
import random
import xml.etree.ElementTree as ET
import time
from collections import defaultdict
from PIL import Image

# æ·±åº¦å­¦ä¹ åº“
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader
    from torchvision import datasets, transforms, models
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ é”™è¯¯: PyTorch ä¸å¯ç”¨")
    sys.exit(1)

# ========================= é…ç½®éƒ¨åˆ† =========================

ROOT = r"D:\ç½‘ç›˜\é¢éƒ¨å£ç½©æ£€æµ‹æ•°æ®é›†"  # æ”¹ä¸ºä½ çš„æ•°æ®è·¯å¾„

IMG_DIR = os.path.join(ROOT, "images")
ANN_DIR = os.path.join(ROOT, "annotations")
DATASET_DIR = "dataset"
MODELS_DIR = "models"

# æ•°æ®é›†é…ç½®
random.seed(42)
torch.manual_seed(42)
CLASS_NAMES = ["with_mask", "without_mask", "mask_incorrect"]

print("=" * 60)
print("ğŸ¯ å£ç½©æ£€æµ‹ - æ·±åº¦å­¦ä¹ ç‰ˆæœ¬")
print("=" * 60)

# ========================= ç¬¬1éƒ¨åˆ†ï¼šæ•°æ®é›†æ„å»º =========================

def parse_annotation(xml_file):
    """è§£æVOCæ ¼å¼XMLæ–‡ä»¶"""
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        filename = root.find('filename').text
        objects = []
        
        for obj in root.findall('object'):
            name = obj.find('name').text
            bndbox = obj.find('bndbox')
            xmin = int(bndbox.find('xmin').text)
            ymin = int(bndbox.find('ymin').text)
            xmax = int(bndbox.find('xmax').text)
            ymax = int(bndbox.find('ymax').text)
            objects.append((name, (xmin, ymin, xmax, ymax)))
        
        return filename, objects
    except Exception as e:
        print(f"è§£æ {xml_file} å¤±è´¥: {e}")
        return None, []

def build_classification_dataset():
    """æ„å»ºåˆ†ç±»æ•°æ®é›†"""
    print("\nğŸ“ æ­£åœ¨æ„å»ºåˆ†ç±»æ•°æ®é›†...")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®
    if not os.path.exists(IMG_DIR):
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {IMG_DIR}")
        return False
    
    if not os.path.exists(ANN_DIR):
        print(f"âŒ æ ‡æ³¨ç›®å½•ä¸å­˜åœ¨: {ANN_DIR}")
        return False
    
    xml_files = [f for f in os.listdir(ANN_DIR) if f.endswith('.xml')]
    print(f"   æ‰¾åˆ° {len(xml_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")
    
    # åˆ›å»ºæ•°æ®é›†ç›®å½•
    if os.path.exists(DATASET_DIR):
        shutil.rmtree(DATASET_DIR)
    os.makedirs(DATASET_DIR)
    
    # æŒ‰ç±»åˆ«åˆ›å»ºç›®å½•
    data_by_split = defaultdict(lambda: defaultdict(list))
    
    for split in ['train', 'val', 'test']:
        for class_name in CLASS_NAMES:
            os.makedirs(os.path.join(DATASET_DIR, split, class_name), exist_ok=True)
    
    # å¤„ç†æ‰€æœ‰XMLæ–‡ä»¶
    processed = 0
    skipped = 0
    
    for xml_file in xml_files:
        xml_path = os.path.join(ANN_DIR, xml_file)
        filename, objects = parse_annotation(xml_path)
        
        if filename is None or not objects:
            skipped += 1
            continue
        
        img_path = os.path.join(IMG_DIR, filename)
        if not os.path.exists(img_path):
            skipped += 1
            continue
        
        try:
            img = Image.open(img_path)
            
            for obj_class, (xmin, ymin, xmax, ymax) in objects:
                if obj_class not in CLASS_NAMES:
                    continue
                
                # è£å‰ªäººè„¸åŒºåŸŸ
                face_img = img.crop((xmin, ymin, xmax, ymax))
                
                # éšæœºåˆ†é…åˆ°train/val/test
                rand = random.random()
                if rand < 0.7:
                    split = 'train'
                elif rand < 0.85:
                    split = 'val'
                else:
                    split = 'test'
                
                # ä¿å­˜å›¾åƒ
                save_dir = os.path.join(DATASET_DIR, split, obj_class)
                save_path = os.path.join(save_dir, f"{processed}_{obj_class}.jpg")
                face_img.save(save_path)
                data_by_split[split][obj_class].append(save_path)
                
            processed += 1
            
            if processed % 100 == 0:
                print(f"   å·²å¤„ç† {processed} å¼ å›¾åƒ...")
        
        except Exception as e:
            skipped += 1
    
    print(f"\nâœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼")
    print(f"   - å·²å¤„ç†: {processed} å¼ ")
    print(f"   - è·³è¿‡: {skipped} å¼ ")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    for split in ['train', 'val', 'test']:
        print(f"\n   {split.upper()}é›†:")
        total = 0
        for class_name in CLASS_NAMES:
            count = len(data_by_split[split][class_name])
            print(f"      {class_name}: {count}")
            total += count
        print(f"      æ€»è®¡: {total}")
    
    return True

# ========================= ç¬¬2éƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹ =========================

def train_resnet18(num_epochs=5):
    """è®­ç»ƒResNet18æ¨¡å‹"""
    print("\nğŸ¤– æ­£åœ¨è®­ç»ƒResNet18æ¨¡å‹ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®å¢å¼ºå’Œè½¬æ¢
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'test': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    }
    
    # åŠ è½½æ•°æ®é›†
    image_datasets = {
        'train': datasets.ImageFolder(os.path.join(DATASET_DIR, 'train'), data_transforms['train']),
        'val': datasets.ImageFolder(os.path.join(DATASET_DIR, 'val'), data_transforms['val']),
        'test': datasets.ImageFolder(os.path.join(DATASET_DIR, 'test'), data_transforms['test'])
    }
    
    dataloaders = {
        'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=0),
        'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=0),
        'test': DataLoader(image_datasets['test'], batch_size=32, shuffle=False, num_workers=0)
    }
    
    # åŠ è½½é¢„è®­ç»ƒçš„ResNet18
    model = models.resnet18(pretrained=True)
    
    # å†»ç»“å‰é¢çš„å±‚
    for param in model.layer1.parameters():
        param.requires_grad = False
    for param in model.layer2.parameters():
        param.requires_grad = False
    
    # ä¿®æ”¹æœ€åçš„å…¨è¿æ¥å±‚
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, len(CLASS_NAMES))
    
    model = model.to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # è®­ç»ƒå¾ªç¯
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    
    for epoch in range(num_epochs):
        print(f"\n   Epoch {epoch + 1}/{num_epochs}")
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for inputs, labels in dataloaders['train']:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            train_correct += (preds == labels).sum().item()
            train_total += labels.size(0)
        
        train_loss /= len(dataloaders['train'])
        train_acc = train_correct / train_total
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, labels in dataloaders['val']:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                val_correct += (preds == labels).sum().item()
                val_total += labels.size(0)
        
        val_loss /= len(dataloaders['val'])
        val_acc = val_correct / val_total
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        print(f"      Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"      Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
    
    # æµ‹è¯•é˜¶æ®µ
    print(f"\n   æ­£åœ¨è¯„ä¼°æµ‹è¯•é›†...")
    model.eval()
    test_correct = 0
    test_total = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for inputs, labels in dataloaders['test']:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            test_correct += (preds == labels).sum().item()
            test_total += labels.size(0)
            all_preds.append(preds.cpu().numpy())
            all_labels.append(labels.cpu().numpy())
    
    import numpy as np
    all_preds = np.concatenate(all_preds)
    all_labels = np.concatenate(all_labels)
    test_acc = test_correct / test_total
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs(MODELS_DIR, exist_ok=True)
    model_path = os.path.join(MODELS_DIR, 'resnet18_mask.pth')
    torch.save(model.state_dict(), model_path)
    print(f"   âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print(f"\n   ğŸ“Š æµ‹è¯•é›†ç»“æœ:")
    print(f"      æ€»ä½“å‡†ç¡®ç‡: {test_acc:.4f}")
    
    for i, class_name in enumerate(CLASS_NAMES):
        mask = all_labels == i
        if np.sum(mask) > 0:
            class_acc = np.sum(all_preds[mask] == all_labels[mask]) / np.sum(mask)
            print(f"      {class_name}: {class_acc:.4f}")
    
    return {
        'model': 'ResNet18',
        'accuracy': test_acc,
        'predictions': all_preds,
        'labels': all_labels
    }

def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # ç¬¬1æ­¥ï¼šæ„å»ºæ•°æ®é›†
    if not build_classification_dataset():
        print("âŒ æ•°æ®é›†æ„å»ºå¤±è´¥")
        return
    
    # ç¬¬2æ­¥ï¼šè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
    if not TORCH_AVAILABLE:
        print("âŒ PyTorchä¸å¯ç”¨")
        return
    
    results = []
    results.append(train_resnet18(num_epochs=5))
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("=" * 60)
    
    elapsed = time.time() - start_time
    print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.2f} ç§’")

if __name__ == "__main__":
    main()
